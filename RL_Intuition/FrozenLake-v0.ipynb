{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>State    Description                    Reward</b>\n",
    "S        Agent's starting pt. - Safe    0\n",
    "F        Frozen surface - Safe          0\n",
    "H        Hole - Game over               0\n",
    "G        Goal - Game over               1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Q-Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1     # alpha\n",
    "discount_rate = 0.99    # gamma\n",
    "\n",
    "exploration_rate = 1    # sigma\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode parameters\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-Exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # exploit\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            # explore\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table for Q(state, action)\n",
    "        a = (1-learning_rate)*q_table[state, action]\n",
    "        # a = (1-alpha)*old_value\n",
    "        b = learning_rate*(reward + discount_rate*np.max(q_table[new_state, :]))\n",
    "        # b = alpha*learned_value\n",
    "        q_table[state, action] = a + b\n",
    "        \n",
    "        # Set new state & update current episode reward\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        # check for hole or goal\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Update Exploration Rate Decay\n",
    "    diff = max_exploration_rate - min_exploration_rate\n",
    "    exploration_rate = min_exploration_rate + diff*np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add final current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Average reward per thousand episodes***\n",
      "1000 :  0.03200000000000002\n",
      "2000 :  0.20400000000000015\n",
      "3000 :  0.4030000000000003\n",
      "4000 :  0.5490000000000004\n",
      "5000 :  0.6470000000000005\n",
      "6000 :  0.6230000000000004\n",
      "7000 :  0.6660000000000005\n",
      "8000 :  0.6760000000000005\n",
      "9000 :  0.6780000000000005\n",
      "10000 :  0.6850000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate & print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"***Average reward per thousand episodes***\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Q-Table***\n",
      "[[0.53655583 0.51845247 0.51081785 0.51127514]\n",
      " [0.38132598 0.22829275 0.39472007 0.51057823]\n",
      " [0.41214766 0.41086179 0.40675767 0.47795336]\n",
      " [0.20693785 0.24013831 0.36635295 0.4595482 ]\n",
      " [0.55665581 0.43697615 0.45059725 0.43911091]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14240114 0.10859987 0.21434851 0.13822023]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3371566  0.44241845 0.42375123 0.59138084]\n",
      " [0.32600981 0.63688327 0.47678581 0.29336225]\n",
      " [0.57797187 0.42539118 0.29216137 0.37807542]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44134909 0.42568543 0.75750115 0.62405326]\n",
      " [0.72505339 0.85814899 0.80843333 0.76756864]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print Updated Q-Table\n",
    "print(\"***Q-Table***\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Reached the goal!\n"
     ]
    }
   ],
   "source": [
    "# Watch the agent play by playing the best action\n",
    "for episode in range(3):\n",
    "    # initialize new episode parameters\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"Episode \", episode+1, \"\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # choose action with highest Q-value for current state i.e., exploit\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        # take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # check for hole or goal\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"\\nReached the goal!\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"\\nFell through a hole!\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        # set new state\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
