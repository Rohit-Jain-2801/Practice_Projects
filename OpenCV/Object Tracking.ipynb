{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering By Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# define range of PURPLE color in HSV\n",
    "lower_purple = np.array([125, 0, 0])\n",
    "upper_purple = np.array([175, 255, 255])\n",
    "\n",
    "# loop until break statement is exectured\n",
    "while True:\n",
    "    # Read webcam image\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Convert image from RBG/BGR to HSV so we easily filter\n",
    "    hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Use inRange to capture only the values between lower & upper purple\n",
    "    mask = cv2.inRange(hsv_img, lower_purple, upper_purple)\n",
    "\n",
    "    # Perform Bitwise AND on mask and our original frame\n",
    "    res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    cv2.imshow('Original', frame)  \n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.imshow('Filtered Color Only', res)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OpenCV 2.4.13 only\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# # Intialize Video\n",
    "# cap = cv2.VideoCapture('images/walking.avi')\n",
    "\n",
    "# # Initlaize background subtractor\n",
    "\n",
    "# # Gaussian Mixture-based Background/Foreground Segmentation Algorithm\n",
    "# foreground_background1 = cv2.BackgroundSubtractorMOG()\n",
    "\n",
    "# # Improved adaptive Gausian mixture model\n",
    "# foreground_background2 = cv2.BackgroundSubtractorMOG2()\n",
    "\n",
    "# # KNN Background Subtractor\n",
    "# foreground_background3 = cv2.BackgroundSubtractorKNN()\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Apply background subtractor to get our foreground mask\n",
    "#     foreground_mask1 = foreground_background1.apply(frame)\n",
    "#     foreground_mask2 = foreground_background2.apply(frame)\n",
    "#     foreground_mask3 = foreground_background3.apply(frame)\n",
    "\n",
    "#     cv2.imshow('Output1', foreground_mask1)\n",
    "#     cv2.imshow('Output2', foreground_mask2)\n",
    "#     cv2.imshow('Output3', foreground_mask3)\n",
    "#     if cv2.waitKey(1) == 13: \n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreground Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initalize webacam and store first frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Create a flaot numpy array with frame values\n",
    "average = np.float32(frame)\n",
    "\n",
    "while True:\n",
    "    # Get webcam frmae\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # 0.01 is the weight of image, play around to see how it changes\n",
    "    cv2.accumulateWeighted(frame, average, 0.01)\n",
    "    \n",
    "    # Scales, calculates absolute values, and converts the result to 8-bit\n",
    "    background = cv2.convertScaleAbs(average)\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "    cv2.imshow('Disapearing Background', background)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meanshift Object Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.calcHist is simply a function calculates the color histograms for an array of images.\n",
    "\n",
    "calcBackProject is a somewhat more complicated function, that calculates the histogram back projection.\n",
    "In this case, the histogram back projection gives a probability estimate an image is equal to the image the original histogram was generated.\n",
    "calcBackProject takes the histogram generated by calcHist and projects it back onto an image. The result is the probability that each pixel belongs to the image that originally generated the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([125, 0, 0])\n",
    "upper_purple = np.array([175, 255, 255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x, y), (x+w, y+h), 255, 2)    \n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camshift Object Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camshift provides variable frame whereas Meanshift provides fixed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([130, 60, 60])\n",
    "upper_purple = np.array([175, 255, 255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "        # apply Camshift to get the new location\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image \n",
    "        # We use polylines to represent Adaptive box \n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame, [pts], True, 255, 2)\n",
    "        cv2.imshow('Camshift Tracking', img2)\n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow Object Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucas-Kanade Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV provides all these in a single function, cv.calcOpticalFlowPyrLK(). Here, we create a simple application which tracks some points in a video. To decide the points, we use cv.goodFeaturesToTrack(). We take the first frame, detect some Shi-Tomasi corner points in it, then we iteratively track those points using Lucas-Kanade optical flow. For the function cv.calcOpticalFlowPyrLK() we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We iteratively pass these next points as previous points in next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load video stream\n",
    "cap = cv2.VideoCapture('images/walking.avi')\n",
    "\n",
    "# Set parameters for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Set parameters for lucas kanade optical flow\n",
    "lucas_kanade_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "# Used to create our trails for object movement in the image \n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, prev_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find inital corner locations\n",
    "prev_corners = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate optical flow\n",
    "    new_corners, status, errors = cv2.calcOpticalFlowPyrLK(prev_gray, \n",
    "                                                           frame_gray, \n",
    "                                                           prev_corners, \n",
    "                                                           None, \n",
    "                                                           **lucas_kanade_params)\n",
    "\n",
    "    # Select and store good points\n",
    "    good_new = new_corners[status==1]\n",
    "    good_old = prev_corners[status==1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "        \n",
    "    img = cv2.add(frame,mask)\n",
    "\n",
    "    # Show Optical Flow\n",
    "    cv2.imshow('Optical Flow - Lucas-Kanade',img)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prev_corners = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow for all the points in the frame. It is based on Gunner Farneback's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video stream\n",
    "cap = cv2.VideoCapture(\"images/walking.avi\")\n",
    "\n",
    "# Get first frame\n",
    "ret, first_frame = cap.read()\n",
    "previous_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(first_frame)\n",
    "hsv[...,1] = 255\n",
    "\n",
    "while True:\n",
    "    # Read of video file\n",
    "    ret, frame2 = cap.read()\n",
    "    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Computes the dense optical flow using the Gunnar Farnebackâ€™s algorithm\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_gray, next, \n",
    "                                        None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # use flow to calculate the magnitude (speed) and angle of motion\n",
    "    # use these values to calculate the color to reflect speed and angle\n",
    "    magnitude, angle = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = angle * (180 / (np.pi/2))\n",
    "    hsv[...,2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    final = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Show our demo of Dense Optical Flow\n",
    "    cv2.imshow('Dense Optical Flow', final)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "    \n",
    "    # Store current image as previous image\n",
    "    previous_gray = next\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
