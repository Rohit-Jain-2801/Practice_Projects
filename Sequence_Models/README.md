# Sequence_Models
Working with sequences!

<br/>

## ⮚ Features-
### ⮩ Description:
* **TextProcessing**
  + Tokenization, Sequences, PaddedSequences, TF-IDFVectorization
* **IMDB_Reviews**
  + Exploring Diff. RNNs, Embeddings + BinaryOutput
* **Emojify**
  + Classifying text into emotions (5 categories)
* **TransferLearning**
  + Using Glove Embeddings for BinaryTextClassification (Positive/Negative)
* **ShakespeareSonnets**
  + Language Modeling - Predicting next word
* **NeuralMachineTranslation**
  + Translator - Seq2Seq, Seq2Seq with Attention (Bahdanau/Luong), Transformer

### ⮩ References:
#### ➝ Papers:
* [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)
* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (GRU)](https://arxiv.org/pdf/1412.3555.pdf)
* [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
* [Linguistic Regularities in Continuous Space Word Representations (Analogies)](https://www.aclweb.org/anthology/N13-1090.pdf)
* [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
* [Efficient Estimation of Word Representations in Vector Space (Skip-Grams)](https://arxiv.org/pdf/1301.3781.pdf)
* [Distributed Representations of Words and Phrases and their Compositionality (Negative Sampling)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
* [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
* [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (Seq-to-Seq)](https://arxiv.org/pdf/1406.1078.pdf)
* [Deep Captioning With Multimodal Recurrent Neural Networks (M-RNN)](https://arxiv.org/pdf/1412.6632.pdf)
* [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf)
* [Deep Visual-Semantic Alignments for Generating Image Descriptions (Image Captioning)](https://arxiv.org/pdf/1412.2306.pdf)
* [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)
* [Neural Machine Translation by Jointly Learning to Align and Translate (Attention Model)](https://arxiv.org/pdf/1409.0473.pdf)
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
* [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks (CTC Cost)](https://www.cs.toronto.edu/~graves/icml_2006.pdf)
* [Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units](https://arxiv.org/pdf/1812.11928.pdf)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
* [Bi-Directional Attention Flow For Machine Comprehension (BiDAF)](https://arxiv.org/pdf/1611.01603.pdf)
* [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733.pdf)
* [Semi-supervised Sequence Learning](https://arxiv.org/pdf/1511.01432.pdf)
* [Deep contextualized word representations (ELMo)](https://arxiv.org/pdf/1802.05365.pdf)
* [Universal Language Model Fine-tuning for Text Classification (ULMFiT)](https://arxiv.org/pdf/1801.06146.pdf)
* [Attention Is All You Need (Transformer)](https://arxiv.org/pdf/1706.03762.pdf)
* [Improving Language Understanding by Generative Pre-Training (OpenAI GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [Language Models are Unsupervised Multitask Learners (OpenAI GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
* [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)
* [ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)
* [Gaussian Error Linear Unit (GELUs)](https://arxiv.org/pdf/1606.08415.pdf)
* [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
* [ELECTRA: Pre-Training Text Encoders as Discriminators Rather than Generators](https://arxiv.org/pdf/2003.10555.pdf)
* [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)
* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)](https://arxiv.org/pdf/1910.10683.pdf)

#### ➝ Blogs & Docs:
* [Medium - Michele Cavaioni](https://medium.com/@mikecavs)
* [Recurrent Neural Networks](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)
* [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
* [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
* [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
* [What is a Recurrent Neural Networks (RNNS) and Gated Recurrent Unit (GRUS)](https://medium.com/@george.drakos62/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69)
* [When to use GRU over LSTM?](https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm)
* [An Introduction to t-SNE with Python Example](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* [Keon - Awesome NLP](https://github.com/keon/awesome-nlp)
* [MorvanZhou - NLP-Tutorials](https://github.com/MorvanZhou/NLP-Tutorials)
* [Minimaxir - textgenrnn](https://github.com/minimaxir/textgenrnn)
* [Jay Alammar Blog](https://jalammar.github.io/)
* [Machine Talk](https://machinetalk.org/)
* [The Super Duper NLP Repo](https://notebooks.quantumstat.com/)
* Understanding 'units' of LSTM keras layer
  + [Keras - LSTM layer](https://keras.io/api/layers/recurrent_layers/lstm/)
  + [What is "units" in LSTM layer of Keras?](https://zhuanlan.zhihu.com/p/58854907)
  + [In Keras, what exactly am I configuring when I create a stateful `LSTM` layer with N `units`?](https://stackoverflow.com/questions/44273249/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi#:~:text=Basically%2C%20the%20unit%20means%20the,be%20unit%20%2Dlength%20as%20well.)
  + [Understanding Keras LSTMs](https://stackoverflow.com/questions/38714959/understanding-keras-lstms/38737941#38737941)
  + [Doubts regarding Understanding Keras LSTMs](https://stackoverflow.com/questions/53955093/doubts-regarding-understanding-keras-lstms)
  + [Keras LSTM tutorial – How to easily build a powerful deep learning language model](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)
* Attention Mechanism
  + [Attention — Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)
  + [Seq2seq pay Attention to Self Attention: Part 1](https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad)
  + [Implementing Neural Machine Translation with Attention mechanism using Tensorflow](https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f)
  + [Essentials of Deep Learning – Sequence to Sequence modelling with Attention (using python)](https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/#:~:text=A%20typical%20sequence%20to%20sequence,an%20encoder%20and%20a%20decoder.&text=So%20when%20such%20an%20input,step%20of%20the%20decoder's%20iteration.)
  + [A Comprehensive Guide to Attention Mechanism in Deep Learning for Everyone](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)
* [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)
* [A Gentle Introduction to Calculating the BLEU Score for Text in Python](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)
* [Zhijing-jin - Bleu](https://github.com/zhijing-jin/bleu)
* TensorFlow
  + [TensorFlow NMT](https://github.com/tensorflow/nmt)
  + [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)
  + [tfa.seq2seq.BahdanauAttention](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BahdanauAttention)
  + [tf.nn.ctc_beam_search_decoder](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder)
  + [tf.keras.layers.Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention)
* [Tensorflow_addons seq2seq example using Attention and Beam Search](https://medium.com/@dhirensk/tensorflow-addons-seq2seq-example-using-attention-and-beam-search-9f463b58bc6b)
* [Theamrzaki - Text_Summurization_Abstractive_Methods](https://github.com/theamrzaki/text_summurization_abstractive_methods)
* Transformers
  + [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
  + [HuggingFace - Transformers](https://huggingface.co/transformers/index.html)
  + [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)
  + [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)
  + [A Transformer Chatbot Tutorial with TensorFlow 2.0](https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2)
  + [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp)
  + [CyberZHG - keras-transformer](https://github.com/CyberZHG/keras-transformer)
  + [Zbloss - TransformerModel](https://github.com/zbloss/TransformerModel)
  + [Luozhouyang - transformers-keras](https://github.com/luozhouyang/transformers-keras)
* [GELU activation](https://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)
* [Google-Research - BERT](https://github.com/google-research/bert)
* [3 subword algorithms help to improve your NLP model performance](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)
* [GLUE Benchmark](https://gluebenchmark.com/)
* [YouTube - ChrisMcCormickAI](https://www.youtube.com/channel/UCoRX98PLOsaN8PtekB9kWrw/videos)
* [Divide Hugging Face Transformers training time by 2 or more with dynamic padding and uniform length batching](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e)
* [OK Google, Tell Me How Trigger Word Detection works](https://medium.com/x8-the-ai-community/ok-google-tell-me-how-trigger-word-detection-works-f6f877e2cd8b)

<br/>

## ⮚ Datasets-
### ⮩ Links:
* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection) (for TextProcessing)
* [Emojify](https://www.kaggle.com/alvinrindra/emojify) (for Emojify)
* [Sonnet](https://www.kaggle.com/jojo096/sonnet) (for ShakespeareSonnets)
* [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/) (for NeuralMachineTranslation)
