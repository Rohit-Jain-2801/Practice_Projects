# Sequence_Models
Working with sequences!

<br/>

## References-
* [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)
* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (GRU)](https://arxiv.org/pdf/1412.3555.pdf)
* [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
* [Linguistic Regularities in Continuous Space Word Representations (Analogies)](https://www.aclweb.org/anthology/N13-1090.pdf)
* [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
* [Efficient Estimation of Word Representations in Vector Space (Skip-Grams)](https://arxiv.org/pdf/1301.3781.pdf)
* [Distributed Representations of Words and Phrases and their Compositionality (Negative Sampling)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
* [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
* [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (Seq-to-Seq)](https://arxiv.org/pdf/1406.1078.pdf)
* [Deep Captioning With Multimodal Recurrent Neural Networks (M-RNN)](https://arxiv.org/pdf/1412.6632.pdf)
* [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf)
* [Deep Visual-Semantic Alignments for Generating Image Descriptions (Image Captioning)](https://arxiv.org/pdf/1412.2306.pdf)
* [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)
* [Neural Machine Translation by Jointly Learning to Align and Translate (Attention Model)](https://arxiv.org/pdf/1409.0473.pdf)
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
* [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks (CTC Cost)](https://www.cs.toronto.edu/~graves/icml_2006.pdf)
* [Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units](https://arxiv.org/pdf/1812.11928.pdf)

<br/>

* [Medium - Michele Cavaioni](https://medium.com/@mikecavs)
* [Recurrent Neural Networks](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)
* [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
* [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
* [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
* [What is a Recurrent Neural Networks (RNNS) and Gated Recurrent Unit (GRUS)](https://medium.com/@george.drakos62/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69)
* [When to use GRU over LSTM?](https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm)
* [An Introduction to t-SNE with Python Example](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* [OK Google, Tell Me How Trigger Word Detection works](https://medium.com/x8-the-ai-community/ok-google-tell-me-how-trigger-word-detection-works-f6f877e2cd8b)
* [Attention — Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)
* [Keon - Awesome NLP](https://github.com/keon/awesome-nlp)
* [MorvanZhou - NLP-Tutorials](https://github.com/MorvanZhou/NLP-Tutorials)
* [Minimaxir - textgenrnn](https://github.com/minimaxir/textgenrnn)
* [Jay Alammar Blog](https://jalammar.github.io/)
* [The Super Duper NLP Repo](https://notebooks.quantumstat.com/)
* Understanding 'units' of LSTM keras layer
  + [Keras - LSTM layer](https://keras.io/api/layers/recurrent_layers/lstm/)
  + [What is "units" in LSTM layer of Keras?](https://zhuanlan.zhihu.com/p/58854907)
  + [In Keras, what exactly am I configuring when I create a stateful `LSTM` layer with N `units`?](https://stackoverflow.com/questions/44273249/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi#:~:text=Basically%2C%20the%20unit%20means%20the,be%20unit%20%2Dlength%20as%20well.)
  + [Understanding Keras LSTMs](https://stackoverflow.com/questions/38714959/understanding-keras-lstms/38737941#38737941)
  + [Doubts regarding Understanding Keras LSTMs](https://stackoverflow.com/questions/53955093/doubts-regarding-understanding-keras-lstms)
  + [Keras LSTM tutorial – How to easily build a powerful deep learning language model](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)
* [Essentials of Deep Learning – Sequence to Sequence modelling with Attention (using python)](https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/#:~:text=A%20typical%20sequence%20to%20sequence,an%20encoder%20and%20a%20decoder.&text=So%20when%20such%20an%20input,step%20of%20the%20decoder's%20iteration.)
